[{"authors":["admin"],"categories":null,"content":"Warning: This website is still under construction. Hi, my name is Hanne Oberman. I\u0026rsquo;m a research master\u0026rsquo;s student Methodology and Statistics for the Behavioural, Biomedical and Social Sciences at Utrecht University. I expect to graduate May 2020.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Warning: This website is still under construction. Hi, my name is Hanne Oberman. I\u0026rsquo;m a research master\u0026rsquo;s student Methodology and Statistics for the Behavioural, Biomedical and Social Sciences at Utrecht University. I expect to graduate May 2020.","tags":null,"title":"Hanne Oberman","type":"authors"},{"authors":null,"categories":["MSc Thesis"],"content":"\rThis file describes how convergence diagnostic \\(\\widehat{R}\\) is computed in the simulation study I ran for my Research Report. \\(\\widehat{R}\\) is also called ‘potential scale reduction factor’ or ‘Gelman-Rubin statistic’. It tells us how much the variance of an estimate could be shrunken down if we would have run infinitely many iterations.\nStep 1: Impute some data\rIt’s easiest to show how \\(\\widehat{R}\\) is computed by applying it on a simple example. Load the nhanes data and impute the variables with missingness.\n# load required packages/functions\rlibrary(magrittr)\rlibrary(mice)\r## Loading required package: lattice\r## ## Attaching package: \u0026#39;mice\u0026#39;\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## cbind, rbind\rsource(\u0026quot;C:/Users/User/Desktop/shinyMice/Simulation/Functions/Convergence.R\u0026quot;)\rsource(\u0026quot;C:/Users/User/Desktop/shinyMice/Simulation/Functions/Convergence_supplement.R\u0026quot;)\r# set parameters\rmaxit \u0026lt;- 10\rn.var \u0026lt;- 3 # run imputation\rimp \u0026lt;- mice(nhanes[,-1], maxit = maxit, print = F)\r\rStep 2: Evaluate the functions to use\rWe are going to compute \\(\\widehat{R}\\) values per imputed variable. As recommended by Vehtari et al. (2019), we compute split-half, rank-normalized \\(\\widehat{R}\\) values for the bulk and the tails of the distribution. For the latter, we need to fold the imputation chains before splitting them in half and rank-normalizing them. Then, \\(\\widehat{R}\\) is computed for both (bulk and tails). The highest of these two \\(\\widehat{R}\\) values is extracted. This is performed for each of the imputed variables, and only the maximum \\(\\widehat{R}\\) value across variables is reported.\nThe rhat_function function is the wrapper that performs all of this. Within the function, we call 4 other functions (explaination below): fold_sims(), split_chains(), z_scale(), and get.rhat().\n# show the contents of the function\rrhat_function\r## function (imp, maxit, n.var = 4) ## {\r## rhat \u0026lt;- matrix(NA, nrow = 1, ncol = n.var)\r## for (v in 1:n.var) {\r## rhat_bulk \u0026lt;- imp$chainMean[v, , ] %\u0026gt;% split_chains() %\u0026gt;% ## z_scale() %\u0026gt;% get.rhat(maxit = maxit)\r## rhat_tail \u0026lt;- imp$chainMean[v, , ] %\u0026gt;% fold_sims() %\u0026gt;% ## split_chains() %\u0026gt;% z_scale() %\u0026gt;% get.rhat(maxit = maxit)\r## rhat[v] \u0026lt;- max(rhat_bulk, rhat_tail)\r## }\r## max(rhat)\r## }\rThe first function inside rhat_function is split_chains(). This ‘chops’ the chains in two, to assess trending within chains.\n# show the contents of the function\rsplit_chains\r## function (sims) ## {\r## niter \u0026lt;- dim(sims)[1]\r## if (niter \u0026lt; 4) ## return(sims)\r## half \u0026lt;- niter/2\r## cbind(sims[1:floor(half), ], sims[ceiling(half + 1):niter, ## ])\r## }\rThen the halved chains are rank-normalized.\n# show the contents of the function\rz_scale\r## function (x) ## {\r## S \u0026lt;- length(x)\r## r \u0026lt;- rank(x, ties.method = \u0026quot;average\u0026quot;)\r## z \u0026lt;- qnorm((r - 1/2)/S)\r## if (!is.null(dim(x))) {\r## z \u0026lt;- array(z, dim = dim(x), dimnames = dimnames(x))\r## }\r## z\r## }\rTo detect non-convergence in the tails, \\(\\widehat{R}\\) is also computed on the folded chains (which are also split and rank-normalized).\n# show the contents of the function\rfold_sims\r## function (sims) ## {\r## abs(sims - median(sims))\r## }\rOnly then do we get to the actual \\(\\widehat{R}\\) function. In the get.rhat function, the following equations are applied:\n\"In the equations below, \\(N\\) is the number of draws per chain, \\(M\\) is the number of chains, and \\(S = MN\\) is the total number of draws from all chains. For each scalar summary of interest \\(\\theta\\), we compute \\(B\\) and \\(W\\), the between- and within-chain variances:\n\\[\\begin{align*}\rB =\\frac{N}{M-1} \\sum_{m=1}^{M}\\left(\\bar{\\theta}^{(\\cdot m)}-\\bar{\\theta}^{(\\cdot \\cdot)}\\right)^{2}, \\text { where } \\bar{\\theta}^{(\\cdot m)}=\\frac{1}{N} \\sum_{n=1}^{N} \\theta^{(n m)}, \\text { and } \\bar{\\theta}^{(\\cdot \\cdot)}=\\frac{1}{M} \\sum_{m=1}^{M} \\bar{\\theta}^{(\\cdot m)} \\\\\rW =\\frac{1}{M} \\sum_{m=1}^{M} s_{j}^{2}, \\text { where } s_{m}^{2}=\\frac{1}{N-1} \\sum_{n=1}^{N}\\left(\\theta^{(n m)}-\\bar{\\theta}^{(\\cdot m)}\\right)^{2}\u0026quot;. \\text{ Vehtari et al., 2019, p. 5} \\end{align*}\\]\nThe between and within chain variances are computed from the chain means of the mids object, that underwent the previous transformations (folding, splitting, rank-normalizing). The average of the chain means per imputation \\(m\\) is \\(\\bar{\\theta}^{(\\cdot m)}\\). The average of these \\(m\\) values is \\(\\bar{\\theta}^{(\\cdot \\cdot)}\\). \\(N\\) is the number of iterations (‘maxit’). So we compute \\(B\\) as maxit * var(apply(sims, 2, mean)). And \\(W\\) is computed as mean(apply(sims, 2, var)).\nTo get \\(\\widehat{R}\\) we need to compute the weighted average of \\(B\\) and \\(W\\), \\(\\widehat{\\operatorname{var}}^{+}\\), and evaluate this against the within chain variance W. The weighted average is is computed as follows:\n\\[\\begin{equation*}\r\\widehat{\\operatorname{var}}^{+}(\\theta | y)=\\frac{N-1}{N} W+\\frac{1}{N} B,\r\\end{equation*}\\]\nThen finally, potential scale reduction factor \\(\\widehat{R}\\) is obtained as:\n\\[\\begin{equation*}\r\\widehat{R}=\\sqrt{\\frac{\\widehat{\\operatorname{var}}^{+}(\\theta | y)}{W}}.\r\\end{equation*}\\]\n\\(\\widehat{R}\\) is computed for each variable with the function ‘get.rhat()’.\n# show the contents of the function\rget.rhat\r## function (sims, maxit = maxit) ## {\r## var_between \u0026lt;- maxit * var(apply(sims, 2, mean))\r## var_within \u0026lt;- mean(apply(sims, 2, var))\r## rhat \u0026lt;- sqrt((var_between/var_within + maxit - 1)/maxit)\r## return(rhat)\r## }\r\rStep 3: Apply these functions to get \\(\\widehat{R}\\) values\rNow we know what’s happening, let’s apply it.\nrhat_function(imp, maxit, n.var = n.var)\r## [1] 1.151591\rThis \\(\\widehat{R}\\) value is nice and high, but I also got values below one. Which is theoretically impossible.\n\rSo my question to you is: How is it possible that we can obtain \\(\\widehat{R}\\) values \u0026lt; 1?\r\r","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573830101,"objectID":"4a901cc550eb7f8b998cf0776d090a71","permalink":"/post/convergence-diagnostic-for-multiple-imputation/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/post/convergence-diagnostic-for-multiple-imputation/","section":"post","summary":"This file describes how convergence diagnostic \\(\\widehat{R}\\) is computed in the simulation study I ran for my Research Report. \\(\\widehat{R}\\) is also called ‘potential scale reduction factor’ or ‘Gelman-Rubin statistic’. It tells us how much the variance of an estimate could be shrunken down if we would have run infinitely many iterations.\nStep 1: Impute some data\rIt’s easiest to show how \\(\\widehat{R}\\) is computed by applying it on a simple example.","tags":["Convergence","MICE","ShinyMICE"],"title":"Convergence Diagnostic for Multiple Imputation","type":"post"}]