[{"authors":["admin"],"categories":null,"content":"Hi, my name is Hanne Oberman. I\u0026rsquo;m a research master\u0026rsquo;s student Methodology and Statistics for the Behavioural, Biomedical and Social Sciences at Utrecht University. I expect to graduate May 2020.\n[Warning: This website is still under construction.]\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi, my name is Hanne Oberman. I\u0026rsquo;m a research master\u0026rsquo;s student Methodology and Statistics for the Behavioural, Biomedical and Social Sciences at Utrecht University. I expect to graduate May 2020.\n[Warning: This website is still under construction.]","tags":null,"title":"Hanne Oberman","type":"authors"},{"authors":[],"categories":[],"content":"","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"6c1dc27f22bea2c661a02343730c0c17","permalink":"/publication/missingthepoint/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/publication/missingthepoint/","section":"publication","summary":"MSc Thesis Methodology and Statistics for the Behavioural, Biomedical and Social Sciences (Utrecht University)","tags":[],"title":"TEST MSc Thesis 'Missing the Point: Non-Convergence in Iterative Imputation Algorithms'","type":"publication"},{"authors":null,"categories":["Miscellaneous"],"content":"\rA list of things I found on the internet and thought to be useful.\nReduce whitespace in printed lists\r# create a list\ralist \u0026lt;- list(a=1,b=2,c=list(d=1:3, e=\u0026quot;hello\u0026quot;))\r# regular print\ralist\r## $a\r## [1] 1\r## ## $b\r## [1] 2\r## ## $c\r## $c$d\r## [1] 1 2 3\r## ## $c$e\r## [1] \u0026quot;hello\u0026quot;\r# shorter print\rprint.simple.list(alist)\r## _ ## a 1 ## b 2 ## c.d1 1 ## c.d2 2 ## c.d3 3 ## c.e hello\rSource: https://twitter.com/coolbutuseless/status/1259972282421739522?s=20\n\rRename columns inside dyplyr::select() call\rlibrary(dplyr)\rmtcars %\u0026gt;% # select some variables *and* rename one\rselect(mpg, cyl, displacement = disp) %\u0026gt;% # look at structure of new object\rglimpse()\r## Rows: 32\r## Columns: 3\r## $ mpg \u0026lt;dbl\u0026gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, ...\r## $ cyl \u0026lt;dbl\u0026gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, ...\r## $ displacement \u0026lt;dbl\u0026gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7...\rSource: https://twitter.com/n_a_gilbert/status/1259866258708840452?s=20\n\rComment out any step in a pipeline\rmtcars %\u0026gt;% # perform some wrangling\rfilter(cyl \u0026gt; 4) %\u0026gt;% group_by(cyl) %\u0026gt;% # comment out the last line\r# summarise(mean_wt = mean(wt)) %\u0026gt;% # no problem with identity()\ridentity()\r## # A tibble: 21 x 11\r## # Groups: cyl [2]\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4\r## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4\r## 3 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1\r## 4 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2\r## 5 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1\r## 6 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4\r## 7 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4\r## 8 17.8 6 168. 123 3.92 3.44 18.9 1 0 4 4\r## 9 16.4 8 276. 180 3.07 4.07 17.4 0 0 3 3\r## 10 17.3 8 276. 180 3.07 3.73 17.6 0 0 3 3\r## # ... with 11 more rows\rSource: https://twitter.com/thomas_neitmann/status/1260111985968119808?s=20\n\rRename objects/variables/etc. within one R chunck only\rCtrl + Alt + Shift + M\rSource: https://twitter.com/sergiouribe/status/1251446686255329283?s=20\n\rSelect text vertically (e.g., geom_line() to geom_point())\rAlt + cursor\r\rAuto-format R script/chunk\rCtrl + Shift + A\r\rRound to nearest tenth, hundred, etc.\rround(\r# some vector\rc(1, 5, 6, 90, 11, 27, pi),\r# to -1 decimal = tenth\rdigits = -1)\r## [1] 0 0 10 90 10 30 0\rSource: https://twitter.com/_ColinFay/status/1250706532762861569?s=20\n\rApply function to columns of dataframe\rlibrary(magrittr)\r# expose contents\rmtcars %$% lm(mpg ~ disp + hp)\r## ## Call:\r## lm(formula = mpg ~ disp + hp)\r## ## Coefficients:\r## (Intercept) disp hp ## 30.73590 -0.03035 -0.02484\r\rRename columns in dataframe at the end of a pipeline\rmtcars %\u0026gt;% # some wrangling\rfilter(cyl \u0026gt; 4) %\u0026gt;% # rename variables\rsetNames(., 1:dim(.)[2])\r## 1 2 3 4 5 6 7 8 9 10 11\r## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4\r## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4\r## 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1\r## 4 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2\r## 5 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1\r## 6 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4\r## 7 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4\r## 8 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4\r## 9 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3\r## 10 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3\r## 11 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3\r## 12 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4\r## 13 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4\r## 14 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4\r## 15 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2\r## 16 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2\r## 17 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4\r## 18 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2\r## 19 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4\r## 20 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6\r## 21 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8\r\rPrint variable names cleanly (i.e., comma separated and without quote marks)\r# print just the names\rpaste(colnames(mtcars), collapse = \u0026quot;, \u0026quot;) \r## [1] \u0026quot;mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\u0026quot;\rSource: https://twitter.com/ludmila_janda/status/1217486240058134528?s=20\n\rSave output of R script/chunk\r# define output file, specify whether to add to existing output\rsink(file=\u0026quot;output.txt\u0026quot;, append = FALSE)\r# run some code\rmtcars %\u0026gt;% glimpse()\r## Rows: 32\r## Columns: 11\r## $ mpg \u0026lt;dbl\u0026gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17...\r## $ cyl \u0026lt;dbl\u0026gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,...\r## $ disp \u0026lt;dbl\u0026gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8,...\r## $ hp \u0026lt;dbl\u0026gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, ...\r## $ drat \u0026lt;dbl\u0026gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3....\r## $ wt \u0026lt;dbl\u0026gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150,...\r## $ qsec \u0026lt;dbl\u0026gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90,...\r## $ vs \u0026lt;dbl\u0026gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,...\r## $ am \u0026lt;dbl\u0026gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,...\r## $ gear \u0026lt;dbl\u0026gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3,...\r## $ carb \u0026lt;dbl\u0026gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1,...\r# save output\rsink()\rSource: https://twitter.com/olivier_klein/status/1203347039133802497?s=20\n\rTurn off scientific notation (e.g., 5.3e-2 to 0.053)\roptions(scipen = 999)\rSource: https://twitter.com/edwardhkennedy/status/1188206942017196032?s=20\n\rList of datasets in R\rSource: https://vincentarelbundock.github.io/Rdatasets/datasets.html\n\r","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589469549,"objectID":"63175fb3d3d4c1892c8f9f6c993bbcb3","permalink":"/post/r/twitter-treasures/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/post/r/twitter-treasures/","section":"post","summary":"A list of things I found on the internet and thought to be useful.\nReduce whitespace in printed lists\r# create a list\ralist \u0026lt;- list(a=1,b=2,c=list(d=1:3, e=\u0026quot;hello\u0026quot;))\r# regular print\ralist\r## $a\r## [1] 1\r## ## $b\r## [1] 2\r## ## $c\r## $c$d\r## [1] 1 2 3\r## ## $c$e\r## [1] \u0026quot;hello\u0026quot;\r# shorter print\rprint.simple.list(alist)\r## _ ## a 1 ## b 2 ## c.","tags":["R","rstats"],"title":"Twitter treasures #rstats","type":"post"},{"authors":[],"categories":[],"content":"","date":1578909180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578909180,"objectID":"932f636797f904423cdd1913beb9a81e","permalink":"/publication/sips2019/","publishdate":"2020-01-13T10:53:00+01:00","relpermalink":"/publication/sips2019/","section":"publication","summary":"Pre-study poster to elicit feedback on my thesis project from SIPS attendees.","tags":[],"title":"Pre-study presentation SIPS 2019","type":"publication"},{"authors":null,"categories":["MSc Thesis"],"content":"\rThis file describes how convergence diagnostic \\(\\widehat{R}\\) is computed in the simulation study I ran for my Research Report. \\(\\widehat{R}\\) is also called ‘potential scale reduction factor’ or ‘Gelman-Rubin statistic’. It tells us how much the variance of an estimate could be shrunken down if we would have run infinitely many iterations.\nStep 1: Impute some data\rIt’s easiest to show how \\(\\widehat{R}\\) is computed by applying it on a simple example. Load the nhanes data and impute the variables with missingness.\n# load required packages/functions\rlibrary(magrittr)\rlibrary(mice)\r## Loading required package: lattice\r## ## Attaching package: \u0026#39;mice\u0026#39;\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## cbind, rbind\rsource(\u0026quot;C:/Users/User/Desktop/shinyMice/Simulation/Functions/Convergence.R\u0026quot;)\rsource(\u0026quot;C:/Users/User/Desktop/shinyMice/Simulation/Functions/Convergence_supplement.R\u0026quot;)\r# set parameters\rmaxit \u0026lt;- 10\rn.var \u0026lt;- 3 # run imputation\rimp \u0026lt;- mice(nhanes[,-1], maxit = maxit, print = F)\r\rStep 2: Evaluate the functions to use\rWe are going to compute \\(\\widehat{R}\\) values per imputed variable. As recommended by Vehtari et al. (2019), we compute split-half, rank-normalized \\(\\widehat{R}\\) values for the bulk and the tails of the distribution. For the latter, we need to fold the imputation chains before splitting them in half and rank-normalizing them. Then, \\(\\widehat{R}\\) is computed for both (bulk and tails). The highest of these two \\(\\widehat{R}\\) values is extracted. This is performed for each of the imputed variables, and only the maximum \\(\\widehat{R}\\) value across variables is reported.\nThe rhat_function function is the wrapper that performs all of this. Within the function, we call 4 other functions (explaination below): fold_sims(), split_chains(), z_scale(), and get.rhat().\n# show the contents of the function\rrhat_function\r## function (imp, maxit, n.var = 4, moment = \u0026quot;mean\u0026quot;) ## {\r## rhat \u0026lt;- numeric(length = n.var)\r## names(rhat) \u0026lt;- attr(imp$chainMean, \u0026quot;dimnames\u0026quot;)[[1]]\r## if (moment == \u0026quot;mean\u0026quot;) {\r## sims \u0026lt;- imp$chainMean\r## }\r## else if (moment == \u0026quot;variance\u0026quot;) {\r## sims \u0026lt;- imp$chainVar\r## }\r## for (v in 1:n.var) {\r## rhat_bulk \u0026lt;- sims[v, , ] %\u0026gt;% split_chains() %\u0026gt;% z_scale() %\u0026gt;% ## get.rhat(maxit = maxit)\r## rhat_tail \u0026lt;- sims[v, , ] %\u0026gt;% fold_sims() %\u0026gt;% split_chains() %\u0026gt;% ## z_scale() %\u0026gt;% get.rhat(maxit = maxit)\r## rhat[v] \u0026lt;- max(rhat_bulk, rhat_tail)\r## }\r## return(rhat)\r## }\rThe first function inside rhat_function is split_chains(). This ‘chops’ the chains in two, to assess trending within chains.\n# show the contents of the function\rsplit_chains\r## function (sims) ## {\r## niter \u0026lt;- dim(sims)[1]\r## if (niter \u0026lt; 4) ## return(sims)\r## half \u0026lt;- niter/2\r## cbind(sims[1:floor(half), ], sims[ceiling(half + 1):niter, ## ])\r## }\rThen the halved chains are rank-normalized.\n# show the contents of the function\rz_scale\r## function (x) ## {\r## S \u0026lt;- length(x)\r## r \u0026lt;- rank(x, ties.method = \u0026quot;average\u0026quot;)\r## z \u0026lt;- qnorm((r - 1/2)/S)\r## if (!is.null(dim(x))) {\r## z \u0026lt;- array(z, dim = dim(x), dimnames = dimnames(x))\r## }\r## z\r## }\rTo detect non-convergence in the tails, \\(\\widehat{R}\\) is also computed on the folded chains (which are also split and rank-normalized).\n# show the contents of the function\rfold_sims\r## function (sims) ## {\r## abs(sims - median(sims))\r## }\rOnly then do we get to the actual \\(\\widehat{R}\\) function. In the get.rhat function, the following equations are applied:\n\"In the equations below, \\(N\\) is the number of draws per chain, \\(M\\) is the number of chains, and \\(S = MN\\) is the total number of draws from all chains. For each scalar summary of interest \\(\\theta\\), we compute \\(B\\) and \\(W\\), the between- and within-chain variances:\n\\[\\begin{align*}\rB =\\frac{N}{M-1} \\sum_{m=1}^{M}\\left(\\bar{\\theta}^{(\\cdot m)}-\\bar{\\theta}^{(\\cdot \\cdot)}\\right)^{2}, \\text { where } \\bar{\\theta}^{(\\cdot m)}=\\frac{1}{N} \\sum_{n=1}^{N} \\theta^{(n m)}, \\text { and } \\bar{\\theta}^{(\\cdot \\cdot)}=\\frac{1}{M} \\sum_{m=1}^{M} \\bar{\\theta}^{(\\cdot m)} \\\\\rW =\\frac{1}{M} \\sum_{m=1}^{M} s_{j}^{2}, \\text { where } s_{m}^{2}=\\frac{1}{N-1} \\sum_{n=1}^{N}\\left(\\theta^{(n m)}-\\bar{\\theta}^{(\\cdot m)}\\right)^{2}\u0026quot;. \\text{ Vehtari et al., 2019, p. 5} \\end{align*}\\]\nThe between and within chain variances are computed from the chain means of the mids object, that underwent the previous transformations (folding, splitting, rank-normalizing). The average of the chain means per imputation \\(m\\) is \\(\\bar{\\theta}^{(\\cdot m)}\\). The average of these \\(m\\) values is \\(\\bar{\\theta}^{(\\cdot \\cdot)}\\). \\(N\\) is the number of iterations (‘maxit’). So we compute \\(B\\) as maxit * var(apply(sims, 2, mean)). And \\(W\\) is computed as mean(apply(sims, 2, var)).\nTo get \\(\\widehat{R}\\) we need to compute the weighted average of \\(B\\) and \\(W\\), \\(\\widehat{\\operatorname{var}}^{+}\\), and evaluate this against the within chain variance W. The weighted average is is computed as follows:\n\\[\\begin{equation*}\r\\widehat{\\operatorname{var}}^{+}(\\theta | y)=\\frac{N-1}{N} W+\\frac{1}{N} B,\r\\end{equation*}\\]\nThen finally, potential scale reduction factor \\(\\widehat{R}\\) is obtained as:\n\\[\\begin{equation*}\r\\widehat{R}=\\sqrt{\\frac{\\widehat{\\operatorname{var}}^{+}(\\theta | y)}{W}}.\r\\end{equation*}\\]\n\\(\\widehat{R}\\) is computed for each variable with the function ‘get.rhat()’.\n# show the contents of the function\rget.rhat\r## function (sims, maxit = maxit) ## {\r## var_between \u0026lt;- maxit * var(apply(sims, 2, mean))\r## var_within \u0026lt;- mean(apply(sims, 2, var))\r## rhat \u0026lt;- sqrt((var_between/var_within + maxit - 1)/maxit)\r## return(rhat)\r## }\r\rStep 3: Apply these functions to get \\(\\widehat{R}\\) values\rNow we know what’s happening, let’s apply it.\nrhat_function(imp, maxit, n.var = n.var)\r## bmi hyp chl ## 1.101684 1.058439 1.155748\rThis \\(\\widehat{R}\\) value is nice and high, but I also got values below one. Which is theoretically impossible.\n\rSo my question to you is: How is it possible that we can obtain \\(\\widehat{R}\\) values \u0026lt; 1?\r\r","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573830101,"objectID":"bc3b061ab6a01691ccbf5af0078e9968","permalink":"/post/thesis/convergence-diagnostic-for-multiple-imputation/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/post/thesis/convergence-diagnostic-for-multiple-imputation/","section":"post","summary":"This file describes how convergence diagnostic \\(\\widehat{R}\\) is computed in the simulation study I ran for my Research Report. \\(\\widehat{R}\\) is also called ‘potential scale reduction factor’ or ‘Gelman-Rubin statistic’. It tells us how much the variance of an estimate could be shrunken down if we would have run infinitely many iterations.\nStep 1: Impute some data\rIt’s easiest to show how \\(\\widehat{R}\\) is computed by applying it on a simple example.","tags":["Convergence","MICE","ShinyMICE"],"title":"Convergence Diagnostic for Multiple Imputation","type":"post"},{"authors":null,"categories":["Courses"],"content":"This project page is dedicated to the course \u0026lsquo;Mark-up Languages and Reproducible Programming\u0026rsquo; (UU).\n","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573776000,"objectID":"1fdadb0c75858d53dbb8d54afb62f59a","permalink":"/project/mlarp/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/project/mlarp/","section":"project","summary":"This project page is dedicated to the course 'Mark-up Languages and Reproducible Programming' (UU).","tags":["MLaRP"],"title":"MLaRP","type":"project"},{"authors":null,"categories":["MSc Thesis"],"content":"","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573776000,"objectID":"51dd0b7f04789f238992393b328c7599","permalink":"/project/shinymice/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/project/shinymice/","section":"project","summary":"Find out more about my MSc thesis on the evaluation of multiply imputed data.","tags":["MICE","ShinyMICE"],"title":"ShinyMICE","type":"project"}]